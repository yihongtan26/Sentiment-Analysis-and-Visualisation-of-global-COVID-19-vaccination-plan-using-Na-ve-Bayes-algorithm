{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f850df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "9084\n",
      "Training time: 0.0488734245300293s\n",
      "Naive Bayes accuracy = 0.9940\n",
      "[[996   4]\n",
      " [  8 992]]\n",
      "Accuracy score: 0.9940\n",
      "Precision score: 0.9960\n",
      "Recall score: 0.9920\n",
      "F1 score: 0.9940\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from textblob import TextBlob\n",
    "import text2emotion as te\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from os import getcwd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "#spell = SpellChecker()\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    " \n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "     \n",
    "##clean symbols\n",
    "def deEmojify(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"                 # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', text)\n",
    "\n",
    "def clean_data(data):\n",
    "    lower_msg = \"\".join([i.lower() for i in data if i not in string.punctuation]) #lower all charracter\n",
    "    msg = deEmojify(lower_msg)                                                    ##clear emoji\n",
    "    msg = re.sub('[^A-Za-z]+', ' ', msg)                                          ##clean symbol\n",
    "    msg = re.sub('(.x?)http.*?(.*?)', ' ', msg)                                   #clean url\n",
    "    return msg\n",
    "\n",
    "def clr_stop_words(words):  ##clear stop words\n",
    "    filter_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filter_words.append(w)\n",
    "    return filter_words  \n",
    "\n",
    "def spell_checker(words):\n",
    "    mispelled = []\n",
    "    for w in words:\n",
    "        text = sym_spell.lookup(w,Verbosity.CLOSEST,max_edit_distance=2, include_unknown=True)\n",
    "        for t in text:\n",
    "            mispelled.append(t._term)\n",
    "            \n",
    "    return mispelled\n",
    "\n",
    "def lemma_words(words):\n",
    "    lemma_word = []\n",
    "    for w in words:\n",
    "        text = lemma.lemmatize(w)\n",
    "        lemma_word.append(text) \n",
    "    return lemma_word   \n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     tweet: a string containing a tweet\n",
    "    # Output:\n",
    "    #     tweets_clean: a list of words containing the processed tweet\n",
    "    # '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def test_lookup(func):\n",
    "    freqs = {('sad', 0): 4,\n",
    "             ('happy', 1): 12,\n",
    "             ('oppressed', 0): 7}\n",
    "    word = 'happy'\n",
    "    label = 1\n",
    "    if func(freqs, word, label) == 12:\n",
    "        return 'SUCCESS!!'\n",
    "    return 'Failed Sanity Check!'\n",
    "\n",
    "\n",
    "def lookup(freqs, word, label):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "    #     word: the word to look up\n",
    "    #     label: the label corresponding to the word\n",
    "    # Output:\n",
    "    #     n: the number of times the word with its corresponding label appears.\n",
    "    # '''\n",
    "    n = 0  # freqs.get((word, label), 0)\n",
    "\n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "\n",
    "    return n\n",
    "\n",
    "#################################################\n",
    "#Process data\n",
    "#Implementing helper functions\n",
    "\n",
    "def count_tweets(result, tweets, ys):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     result: a dictionary that will be used to map each pair to its frequency\n",
    "    #     tweets: a list of tweets\n",
    "    #     ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    # Output:\n",
    "    #     result: a dictionary mapping each pair to its frequency\n",
    "    # '''\n",
    "\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word,y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result\n",
    "\n",
    "##################################################\n",
    "#Train model using naive bayes \n",
    "freqs = count_tweets({}, train_x, train_y)\n",
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    # '''\n",
    "    # Input: \n",
    "    #     freqs: dictionary from (word, label) to how often the word appears\n",
    "    #     train_x: a list of tweets\n",
    "    #     train_y: a list of labels correponding to the tweets (0,1)\n",
    "    # Output: \n",
    "    #     logprior: the log prior. (equation 3 above)\n",
    "    #     loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    # '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0 \n",
    "    \n",
    "   \n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    \n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos=N_neg=V_pos=V_neg=0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos += 1\n",
    "            \n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "        \n",
    "        # else, the label is negative\n",
    "        else: \n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "            \n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = train_y.shape[0]\n",
    "    \n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = train_y[train_y == 1].shape[0]\n",
    "    \n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = train_y[train_y == 0].shape[0]\n",
    "    \n",
    "    # Calculate logprior\n",
    "    logprior  = np.log(D_pos / D) - np.log(D_neg / D)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word, 1), 0)\n",
    "        freq_neg = freqs.get((word, 0), 0)\n",
    "        \n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "     \n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "\n",
    "    \n",
    "    return logprior, loglikelihood\n",
    "start = time.time()\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "stop = time.time()\n",
    "print(logprior)\n",
    "print(len(loglikelihood))\n",
    "\n",
    "###############################################\n",
    "#Test naive bayes\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     tweet: a string\n",
    "    #     logprior: a number\n",
    "    #     loglikelihood: a dictionary of words mapping to numbers\n",
    "    # Output:\n",
    "    #     p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    # '''\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "# # Experiment with your own tweet.\n",
    "# my_tweet = 'She smiled.'\n",
    "# p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "# print('The expected output is', p)\n",
    "\n",
    "y_hats = []\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    # \"\"\"\n",
    "    # Input:\n",
    "    #     test_x: A list of tweets\n",
    "    #     test_y: the corresponding labels for the list of tweets\n",
    "    #     logprior: the logprior\n",
    "    #     loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    # Output:\n",
    "    #     accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    # \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = sum(abs(test_y-y_hats))/len(y_hats)\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "#     # print( '%s -> %f' % (tweet, naive_bayes_predict(tweet, logprior, loglikelihood)))\n",
    "#     p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "# #     print(f'{tweet} -> {p:.2f} ({p_category})')\n",
    "#     print(f'{tweet} -> {p:.2f}')\n",
    "    \n",
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "\n",
    "######################################\n",
    "#Filter words by ratio of positive to negative counts\n",
    "\n",
    "def get_ratio(freqs, word):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     freqs: dictionary containing the words\n",
    "    #     word: string to lookup\n",
    "\n",
    "    # Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "    #     Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    # '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    # use lookup() to find positive counts for the word (denoted by the integer 1)\n",
    "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
    "\n",
    "    # use lookup() to find negative counts for the word (denoted by integer 0)\n",
    "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive']+1)/(pos_neg_ratio['negative']+1)\n",
    "    return pos_neg_ratio\n",
    "\n",
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    # '''\n",
    "    # Input:\n",
    "    #     freqs: dictionary of words\n",
    "    #     label: 1 for positive, 0 for negative\n",
    "    #     threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    # Output:\n",
    "    #     word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "    #     example of a key value pair:\n",
    "    #     {'happi':\n",
    "    #         {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    #     }\n",
    "    # '''\n",
    "    word_list = {}\n",
    "\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio']>= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "\n",
    "        # otherwise, do not include this word in the list (do nothing)\n",
    "\n",
    "    return word_list\n",
    "\n",
    "# Test  function: find negative words at or below a threshold\n",
    "get_words_by_threshold(freqs, label=0, threshold=0.05)\n",
    "\n",
    "# Test  function; find positive words at or above a threshold\n",
    "get_words_by_threshold(freqs, label=1, threshold=10)\n",
    "\n",
    "##############################################\n",
    "# #Error analysis\n",
    "# # Some error analysis done for you\n",
    "# print('Truth Predicted Tweet')\n",
    "# for x, y in zip(test_x, test_y):\n",
    "#     y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "#     if y != (np.sign(y_hat) > 0):\n",
    "#         print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "#             process_tweet(x)).encode('ascii', 'ignore')))\n",
    "\n",
    "acscore = []\n",
    "acprecision = []\n",
    "acrecall = []\n",
    "acF1 = []\n",
    "acTrainingTime = []\n",
    "\n",
    "trainingTime = stop - start\n",
    "print(f\"Training time: {trainingTime}s\")\n",
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n",
    "\n",
    "\n",
    "# Making the confusion matrix and calculating accuracy score\n",
    "cm = confusion_matrix(test_y, y_hats)\n",
    "ac = accuracy_score(test_y, y_hats)\n",
    "precision = precision_score(test_y, y_hats)\n",
    "recall = recall_score(test_y, y_hats)\n",
    "f1 = f1_score(test_y, y_hats)\n",
    "\n",
    "acscore.append(ac)\n",
    "acprecision.append(precision)\n",
    "acrecall.append(recall)\n",
    "acF1.append(f1)\n",
    "acTrainingTime.append(trainingTime)\n",
    "\n",
    "print(cm)\n",
    "print('Accuracy score: {0:0.4f}'.format(ac))\n",
    "print('Precision score: {0:0.4f}'.format(precision))\n",
    "print('Recall score: {0:0.4f}'.format(recall))\n",
    "print('F1 score: {0:0.4f}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbb7369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     ZBook\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tkinter import *\n",
    "import re\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def clearAll() :\n",
    "\tnegativeField.delete(0, END)\n",
    "\tneutralField.delete(0, END)\n",
    "\tpositiveField.delete(0, END)\n",
    "\toverallField.delete(0, END)\n",
    "\tpolarityField.delete(0, END)\n",
    "\ttextArea.delete(1.0, END)\n",
    "\n",
    "def clearResult() :\n",
    "\tnegativeField.delete(0, END)\n",
    "\tneutralField.delete(0, END)\n",
    "\tpositiveField.delete(0, END)\n",
    "\toverallField.delete(0, END)\n",
    "\tpolarityField.delete(0, END)\n",
    "\n",
    "def detect_sentiment():\n",
    "\t# get a whole input content from text box\n",
    "\tsentence = textArea.get(\"1.0\", \"end\")\n",
    "\t\n",
    "\t# Create a SentimentIntensityAnalyzer object.\n",
    "\tsid_obj = SentimentIntensityAnalyzer()\n",
    "\t\n",
    "\t# polarity_scores method of SentimentIntensityAnalyzer\n",
    "\t# object gives a sentiment dictionary.\n",
    "\t# which contains pos, neg, neu, and compound scores.\n",
    "\tsentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\t\n",
    "\tmsgValue = sentence.lower()\n",
    "\tmsgValue = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", msgValue)\n",
    "\tmsgValue = \" \".join([word for word in msgValue.split() if word not in (stop)])\n",
    "\t\n",
    "\tp = naive_bayes_predict(msgValue, logprior, loglikelihood)\n",
    "\tpolarityField.insert(10, p)\n",
    "\t\n",
    "\tstring = str(round(sentiment_dict['neg']*100,2)) + \"% Negative\"\n",
    "\tnegativeField.insert(10, string)\n",
    "\t\n",
    "\tstring = str(round(sentiment_dict['neu']*100,2)) + \"% Neutral\"\n",
    "\tneutralField.insert(10, string)\n",
    "\t\n",
    "\tstring = str(round(sentiment_dict['pos']*100,2)) +\"% Positive\"\n",
    "\tpositiveField.insert(10, string)\n",
    "\t\n",
    "\t# decide sentiment as positive, negative and neutral\n",
    "\tif sentiment_dict['compound'] >= 0.05 :\n",
    "\t\tstring = \"Positive\"\n",
    "\t\n",
    "\telif sentiment_dict['compound'] <= - 0.05 :\n",
    "\t\tstring = \"Negative\"\n",
    "\t\n",
    "\telse :\n",
    "\t\tstring = \"Neutral\"\n",
    "\t\n",
    "\toverallField.insert(10, string)\n",
    "\n",
    "# Driver Code\n",
    "if __name__ == \"__main__\" :\n",
    "\t\n",
    "\t# Create a GUI window\n",
    "\tgui = Tk()\n",
    "\tgui.config(background = \"Light Blue\")\n",
    "\tgui.title(\"Sentiment Detector\")\n",
    "\tgui.geometry(\"360x615\")\n",
    "    \n",
    "\t# create a label : Enter Your Task\n",
    "\tenterText = Label(gui, text = \"Enter Your Sentence\", bg = \"light blue\", font=20)\n",
    "\n",
    "\t# create a text area for the root\n",
    "\t# with lunida 13 font\n",
    "\t# text area is for writing the content\n",
    "\ttextArea = Text(gui, height = 5, width = 37, font = \"lucida 13\")\n",
    "\n",
    "\t# create a Submit Button\n",
    "\tcheck = Button(gui, text = \"Check Sentiment\", fg = \"White\", bg = \"Green\", \n",
    "                   command = lambda:[clearResult(),detect_sentiment()],\n",
    "                   height = 1, width = 30,font=1)\n",
    "\n",
    "\t# Create a negative : label\n",
    "\tpolarity = Label(gui, text = \"Sentence Polarity: \", bg = \"light blue\", font=20)\n",
    "\tnegative = Label(gui, text = \"Sentence Negativity: \", bg = \"light blue\", font=20)\n",
    "\tneutral = Label(gui, text = \"Sentence Neutrality: \", bg = \"light blue\", font=20)\n",
    "\tpositive = Label(gui, text = \"Sentence Positivity: \", bg = \"light blue\", font=20)\n",
    "\toverall = Label(gui, text = \"Sentence Overall Sentiment: \", bg = \"light blue\", font=20)\n",
    "\n",
    "\t# create text entry box\n",
    "\tpolarityField = Entry(gui,width=20,font=20,justify='center',background='light blue')\n",
    "\tnegativeField = Entry(gui,width=20,font=20,justify='center',background='light blue')\n",
    "\tneutralField = Entry(gui,width=20,font=20,justify='center',background='light blue')\n",
    "\tpositiveField = Entry(gui,width=20,font=20,justify='center',background='light blue')\n",
    "\toverallField = Entry(gui,width=17,font=('Georgia 20'),justify='center',background='White')\n",
    "\n",
    "\t# create Buttons\n",
    "\tclear = Button(gui, text = \"Reset\", fg = \"Black\", bg = \"Pink\", command = clearAll, height = 1, width = 15)\n",
    "\tExit = Button(gui, text = \"Exit\", fg = \"White\", bg = \"Red\", command = gui.destroy, height = 1, width = 15)\n",
    "\n",
    "\t# grid method is used for placing\n",
    "\t# the widgets at respective positions\n",
    "\t# in table like structure.\n",
    "\tenterText.grid(row = 1, column = 2)\n",
    "\t\n",
    "\ttextArea.grid(row = 2, column = 2, padx = 10, sticky = W)\n",
    "\t\n",
    "\tcheck.grid(row = 3, column = 2)\n",
    "\t\n",
    "\tpolarity.grid(row = 5, column = 2)\n",
    "\tpolarityField.grid(row = 6, column = 2)\n",
    "    \n",
    "\tnegative.grid(row = 7, column = 2)\n",
    "\tnegativeField.grid(row = 8, column = 2)\n",
    "    \n",
    "\tneutral.grid(row = 9, column = 2)\n",
    "\tneutralField.grid(row = 10, column = 2)\n",
    "    \n",
    "\tpositive.grid(row = 11, column = 2)\n",
    "\tpositiveField.grid(row = 12, column = 2)\n",
    "    \n",
    "\toverall.grid(row = 14, column = 2)\n",
    "\toverallField.grid(row = 15, column = 2)\n",
    "\n",
    "\tclear.grid(row = 17, column = 2)\n",
    "\tExit.grid(row = 18, column = 2)\n",
    "\n",
    "\tcol_count, row_count = gui.grid_size()\n",
    "\n",
    "\tfor col in range(col_count):\n",
    "\t\tgui.grid_columnconfigure(col, minsize=0)\n",
    "\n",
    "\tfor row in range(row_count):\n",
    "\t\tgui.grid_rowconfigure(row, minsize=20)\n",
    "    \n",
    "\t# start the GUI\n",
    "\tgui.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
